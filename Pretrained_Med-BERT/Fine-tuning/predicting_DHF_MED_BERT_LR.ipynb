{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql1izFRPnJcA",
        "outputId": "0110b99a-879a-4e75-db98-68b12210dd6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "# Predicting Diabetes patient risk to develop Heart Failure with Med-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "outputs": [],
      "source": [
        "### Required Packages\n",
        "from termcolor import colored\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import pickle as pkl\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import tqdm\n",
        "import time\n",
        "import transformers\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "%matplotlib inline\n",
        "use_cuda = torch.cuda.is_available()\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "#### Load Data from pickled list\n",
        "\n",
        "The pickled list is a list of lists where each sublist represent a patient record that looks like\n",
        "[pt_id,label, seq_list , segment_list ]\n",
        "where\n",
        "    Label: 1: pt developed HF (case) , 0 control\n",
        "    seq_list: list of all medical codes in all visits\n",
        "    segment list: the visit number mapping to each code in the sequence list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#우리가 가지고 있는 파일 기준으로 변경\n",
        "train_f = pkl.load(open('/content/drive/MyDrive/eICU/my_bertft.train.pkl', 'rb'))\n",
        "valid_f = pkl.load(open('/content/drive/MyDrive/eICU/my_bertft.valid.pkl', 'rb'))\n",
        "test_f  = pkl.load(open('/content/drive/MyDrive/eICU/my_bertft.test.pkl', 'rb'))\n",
        "test_f2 = pkl.load(open('/content/drive/MyDrive/eICU/my_bertft.test_f2.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "14pPjeTWyITn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_f[0] #시퀀스 형태:진단 코드, segmentinfo, label(랜던하게 설정)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPF9pN6eQMql",
        "outputId": "b275fb7c-8e72-4df7-dfc3-811da871f554"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 18, 17, 7, 11, 3], [1, 276, 83, 1, 39, 83], 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ehrseq_to_text(seq):  #tokenizer로 진단 코드 시퀀스를 텍스트로 변환\n",
        "    if isinstance(seq, int):\n",
        "        seq = [seq]\n",
        "    return \" \".join(map(str, seq))"
      ],
      "metadata": {
        "id": "S-2E7kQm6iNX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vbskIgpoKbyf"
      },
      "outputs": [],
      "source": [
        "### Below are key functions for  Data prepartion ,formating input data into features, and model defintion\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "\n",
        "\n",
        "\n",
        "def convert_EHRexamples_to_features(examples,tokenizer, max_seq_length):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for ex in examples:\n",
        "        seq = ex[0]        # 진단코드 시퀀스\n",
        "        label = ex[2]      # label (0/1)\n",
        "        seq_text = ehrseq_to_text(seq)\n",
        "        tokens = tokenizer(\n",
        "          seq_text,\n",
        "          max_length=max_seq_length,\n",
        "          truncation=True,\n",
        "          padding='max_length',\n",
        "          return_tensors='pt'\n",
        "        )\n",
        "        features.append({\n",
        "          \"input_ids\": tokens['input_ids'].squeeze(0),\n",
        "          \"attention_mask\": tokens['attention_mask'].squeeze(0),\n",
        "          \"labels\": torch.tensor(int(label), dtype=torch.long)  # 반드시 int/scalar!\n",
        "        })\n",
        "    return features\n",
        "\n",
        "### This is the EHR version\n",
        "\n",
        "def convert_singleEHR_example(ex_index, example, max_seq_length): #진단 코드 시퀀스 그대로 사용할 때\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        return InputFeatures(\n",
        "            input_ids=[0] * max_seq_length,\n",
        "            input_mask=[0] * max_seq_length,\n",
        "            segment_ids=[0] * max_seq_length,\n",
        "            label_id=0,\n",
        "            is_real_example=False\n",
        "        )\n",
        "\n",
        "    input_ids = example[0]\n",
        "    segment_ids = example[1]\n",
        "    label_id = example[2]\n",
        "\n",
        "    # Left truncate\n",
        "    input_ids = input_ids[-max_seq_length:]\n",
        "    segment_ids = segment_ids[-max_seq_length:]\n",
        "\n",
        "    # 각 길이 확인, 부족하면 개별적으로 패딩\n",
        "    pad_len_ids = max_seq_length - len(input_ids)\n",
        "    pad_len_seg = max_seq_length - len(segment_ids)\n",
        "\n",
        "    if pad_len_ids > 0:\n",
        "        input_ids += [0] * pad_len_ids\n",
        "    if pad_len_seg > 0:\n",
        "        segment_ids += [0] * pad_len_seg\n",
        "\n",
        "    # input_mask: 진짜 토큰엔 1, 패딩엔 0 (input_ids 기준)\n",
        "    input_mask = [1 if i < len(example[0]) else 0 for i in range(max_seq_length)]\n",
        "\n",
        "    assert len(input_ids) == max_seq_length, f\"input_ids: {len(input_ids)}\"\n",
        "    assert len(segment_ids) == max_seq_length, f\"segment_ids: {len(segment_ids)}\"\n",
        "    assert len(input_mask) == max_seq_length, f\"input_mask: {len(input_mask)}\"\n",
        "\n",
        "    feature = [input_ids, input_mask, segment_ids, label_id, True]\n",
        "    return feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R22ovW0VKbyg"
      },
      "outputs": [],
      "source": [
        "class BERTdataEHR(Dataset): #변경\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx]\n",
        "\n",
        "\n",
        "#customized parts for EHRdataloader\n",
        "def my_collate(batch): #안씀\n",
        "        all_input_ids = []\n",
        "        all_input_mask = []\n",
        "        all_segment_ids = []\n",
        "        all_label_ids = []\n",
        "\n",
        "        for feature in batch:\n",
        "            all_input_ids.append(feature[0])\n",
        "            all_input_mask.append(feature[1])\n",
        "            all_segment_ids.append(feature[2])\n",
        "            all_label_ids.append(feature[3])\n",
        "        return [all_input_ids, all_input_mask,all_segment_ids,all_label_ids]\n",
        "\n",
        "\n",
        "class BERTdataEHRloader(DataLoader): #안씀\n",
        "    def __init__(self, dataset, batch_size=128, shuffle=False, sampler=None, batch_sampler=None,\n",
        "                 num_workers=0, collate_fn=my_collate, pin_memory=False, drop_last=False,\n",
        "                 timeout=0, worker_init_fn=None):\n",
        "        DataLoader.__init__(self, dataset, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None,\n",
        "                 num_workers=0, collate_fn=my_collate, pin_memory=False, drop_last=False,\n",
        "                 timeout=0, worker_init_fn=None)\n",
        "        self.collate_fn = collate_fn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train,eval\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        input_ids = batch['input_ids'].cuda() if use_cuda else batch['input_ids']\n",
        "        attention_mask = batch['attention_mask'].cuda() if use_cuda else batch['attention_mask']\n",
        "        labels = batch['labels'].cuda() if use_cuda else batch['labels']\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def eval_auc(model, loader):\n",
        "    model.eval()\n",
        "    y_real = []\n",
        "    y_hat = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].cuda() if use_cuda else batch['input_ids']\n",
        "            attention_mask = batch['attention_mask'].cuda() if use_cuda else batch['attention_mask']\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = torch.sigmoid(outputs.logits).detach().cpu().numpy()\n",
        "            # 이진 분류일 때:\n",
        "            y_hat.extend(probs[:,1])\n",
        "            y_real.extend(labels)\n",
        "    auc = roc_auc_score(y_real, y_hat)\n",
        "    return auc"
      ],
      "metadata": {
        "id": "V6VAILDNqs2U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dO6xktGKbyg"
      },
      "source": [
        "##### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zC-WfPxFKbyh",
        "outputId": "cf0cf932-b2f7-4eb2-c1f8-d2934c2f0a09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 27/27 [00:21<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 0.7217 | Val AUC: 0.4976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:22<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Train Loss: 0.7019 | Val AUC: 0.5070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:22<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Train Loss: 0.6925 | Val AUC: 0.5185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:22<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 | Train Loss: 0.6968 | Val AUC: 0.4871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:23<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 | Train Loss: 0.6908 | Val AUC: 0.4867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:23<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 | Train Loss: 0.6917 | Val AUC: 0.4819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:23<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 | Train Loss: 0.6888 | Val AUC: 0.4959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:23<00:00,  1.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 | Train Loss: 0.6842 | Val AUC: 0.4809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:24<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 | Train Loss: 0.6817 | Val AUC: 0.4821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:24<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 | Train Loss: 0.6722 | Val AUC: 0.5183\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Charangan/MedBERT\")\n",
        "MAX_SEQ_LENGTH = 64\n",
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 1e-5\n",
        "bert_config_file= '/content/drive/MyDrive/eICU/config.json'\n",
        "\n",
        "results=[]\n",
        "\n",
        "#### Data Preparation\n",
        "train_features = convert_EHRexamples_to_features(train_f,tokenizer, MAX_SEQ_LENGTH)\n",
        "test_features = convert_EHRexamples_to_features(test_f, tokenizer, MAX_SEQ_LENGTH)\n",
        "test_features2 = convert_EHRexamples_to_features(test_f2, tokenizer, MAX_SEQ_LENGTH)\n",
        "valid_features = convert_EHRexamples_to_features(valid_f, tokenizer, MAX_SEQ_LENGTH)\n",
        "train = BERTdataEHR(train_features)\n",
        "test = BERTdataEHR(test_features)\n",
        "test2 = BERTdataEHR(test_features2)\n",
        "valid = BERTdataEHR(valid_features)\n",
        "\n",
        "#huggingface 스타일을 위해서 dataloader 변경\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test, batch_size=BATCH_SIZE)\n",
        "test2_loader = DataLoader(test2, batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Charangan/MedBERT\", num_labels=2) #huggingface에서 제공된 사전학습 모델\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "#학습 루프 간소화\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    val_auc = eval_auc(model, valid_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val AUC: {val_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_auc = eval_auc(model, test_loader)\n",
        "test2_auc = eval_auc(model, test2_loader)\n",
        "print(f\"Test1 AUC: {test_auc:.4f} | Test2 AUC: {test2_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVt6VVs4R8U7",
        "outputId": "76437990-5456-46b2-f4f7-e658fb6b2359"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test1 AUC: 0.4872 | Test2 AUC: 0.4872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "fO-d0pKLl-H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh3KP_6WKbyg"
      },
      "outputs": [],
      "source": [
        "class EHR_BERT_LR(nn.Module):\n",
        "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False ,time=False, preTrainEmb=''):\n",
        "        super(EHR_BERT_LR, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout_r = dropout_r\n",
        "        self.cell_type = cell_type\n",
        "        self.preTrainEmb=preTrainEmb\n",
        "        self.time=time\n",
        "\n",
        "        if bi: self.bi=2\n",
        "        else: self.bi=1\n",
        "\n",
        "        self.PreBERTmodel=BertForSequenceClassification.from_pretrained('')\n",
        "        if use_cuda:\n",
        "           self.PreBERTmodel.cuda()\n",
        "        input_size=self.PreBERTmodel.bert.config.vocab_size\n",
        "        self.in_size= self.PreBERTmodel.bert.config.hidden_size\n",
        "\n",
        "        self.dropout = nn.Dropout(p=self.dropout_r)\n",
        "        self.out = nn.Linear(self.in_size,1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax=nn.Softmax()\n",
        "        if use_cuda:\n",
        "            self.flt_typ=torch.cuda.FloatTensor\n",
        "            self.lnt_typ=torch.cuda.LongTensor\n",
        "        else:\n",
        "            self.lnt_typ=torch.LongTensor\n",
        "            self.flt_typ=torch.FloatTensor\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        token_t=torch.from_numpy(np.asarray(sequence[0],dtype=int)).type(self.lnt_typ)\n",
        "        seg_t=torch.from_numpy(np.asarray(sequence[2],dtype=int)).type(self.lnt_typ)\n",
        "        Label_t=torch.from_numpy(np.asarray(sequence[3],dtype=int)).type(self.lnt_typ)\n",
        "        Bert_out=self.PreBERTmodel.bert(input_ids=token_t, attention_mask=torch.from_numpy(np.asarray(sequence[1],dtype=int)).type(self.lnt_typ),\n",
        "                                    token_type_ids=seg_t)\n",
        "        output=self.sigmoid(self.out(Bert_out[1]))\n",
        "        return output.squeeze(),Label_t.type(self.flt_typ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3mCh79Kbyh"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(results)\n",
        "df.columns=['Model','Run','Train_size','Test_size','Valid_size','Train_AUC','Valid_AUC','Test_AUC1','Test_AUC2','Best_Epoch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op3tIpXOKbyh",
        "outputId": "9fbcaa25-2edb-4eb7-e18f-0da3b8a6dfda"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Run</th>\n",
              "      <th>Train_size</th>\n",
              "      <th>Test_size</th>\n",
              "      <th>Valid_size</th>\n",
              "      <th>Train_AUC</th>\n",
              "      <th>Valid_AUC</th>\n",
              "      <th>Test_AUC1</th>\n",
              "      <th>Test_AUC2</th>\n",
              "      <th>Best_Epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>0</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.894582</td>\n",
              "      <td>0.834894</td>\n",
              "      <td>0.830015</td>\n",
              "      <td>0.827995</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>1</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.869437</td>\n",
              "      <td>0.832315</td>\n",
              "      <td>0.828231</td>\n",
              "      <td>0.827004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>2</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.891686</td>\n",
              "      <td>0.833559</td>\n",
              "      <td>0.829083</td>\n",
              "      <td>0.826013</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>3</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.895434</td>\n",
              "      <td>0.832507</td>\n",
              "      <td>0.829849</td>\n",
              "      <td>0.826455</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>4</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.894967</td>\n",
              "      <td>0.832715</td>\n",
              "      <td>0.828891</td>\n",
              "      <td>0.825106</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>5</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.897067</td>\n",
              "      <td>0.833284</td>\n",
              "      <td>0.827417</td>\n",
              "      <td>0.824605</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>6</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.891813</td>\n",
              "      <td>0.833898</td>\n",
              "      <td>0.830488</td>\n",
              "      <td>0.829852</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>7</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.891647</td>\n",
              "      <td>0.835480</td>\n",
              "      <td>0.828714</td>\n",
              "      <td>0.825346</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>8</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.890096</td>\n",
              "      <td>0.837167</td>\n",
              "      <td>0.830588</td>\n",
              "      <td>0.827677</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bert only</td>\n",
              "      <td>9</td>\n",
              "      <td>49999</td>\n",
              "      <td>15000</td>\n",
              "      <td>7500</td>\n",
              "      <td>0.872332</td>\n",
              "      <td>0.835150</td>\n",
              "      <td>0.829565</td>\n",
              "      <td>0.825403</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Model  Run  Train_size  Test_size  Valid_size  Train_AUC  Valid_AUC  \\\n",
              "0  Bert only    0       49999      15000        7500   0.894582   0.834894   \n",
              "1  Bert only    1       49999      15000        7500   0.869437   0.832315   \n",
              "2  Bert only    2       49999      15000        7500   0.891686   0.833559   \n",
              "3  Bert only    3       49999      15000        7500   0.895434   0.832507   \n",
              "4  Bert only    4       49999      15000        7500   0.894967   0.832715   \n",
              "5  Bert only    5       49999      15000        7500   0.897067   0.833284   \n",
              "6  Bert only    6       49999      15000        7500   0.891813   0.833898   \n",
              "7  Bert only    7       49999      15000        7500   0.891647   0.835480   \n",
              "8  Bert only    8       49999      15000        7500   0.890096   0.837167   \n",
              "9  Bert only    9       49999      15000        7500   0.872332   0.835150   \n",
              "\n",
              "   Test_AUC1  Test_AUC2  Best_Epoch  \n",
              "0   0.830015   0.827995           2  \n",
              "1   0.828231   0.827004           1  \n",
              "2   0.829083   0.826013           2  \n",
              "3   0.829849   0.826455           2  \n",
              "4   0.828891   0.825106           2  \n",
              "5   0.827417   0.824605           2  \n",
              "6   0.830488   0.829852           2  \n",
              "7   0.828714   0.825346           2  \n",
              "8   0.830588   0.827677           2  \n",
              "9   0.829565   0.825403           1  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kLKeiMPKbyi"
      },
      "outputs": [],
      "source": [
        "#df.to_csv('DHF_RNN_multirun_shuffled_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhkPtxm0Kbyi"
      },
      "outputs": [],
      "source": [
        "desc2=df[['Model','Train_size','Test_AUC1']].groupby(['Model','Train_size']).describe()\n",
        "desc3=df[['Model','Train_size','Test_AUC2']].groupby(['Model','Train_size']).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QymCEEAIKbyi",
        "outputId": "3d9dccaa-93bd-455d-e62c-f56e3b8f4943"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">Test_AUC2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th>Train_size</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bert only</th>\n",
              "      <th>49999</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.826546</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.824605</td>\n",
              "      <td>0.82536</td>\n",
              "      <td>0.826234</td>\n",
              "      <td>0.827509</td>\n",
              "      <td>0.829852</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Test_AUC2                                         \\\n",
              "                         count      mean       std       min      25%   \n",
              "Model     Train_size                                                    \n",
              "Bert only 49999           10.0  0.826546  0.001613  0.824605  0.82536   \n",
              "\n",
              "                                                    \n",
              "                           50%       75%       max  \n",
              "Model     Train_size                                \n",
              "Bert only 49999       0.826234  0.827509  0.829852  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "desc3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_37_env",
      "language": "python",
      "name": "py_37_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}